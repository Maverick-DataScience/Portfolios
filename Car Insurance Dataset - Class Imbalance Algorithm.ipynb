{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39a3a6f",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"font-family: 'Georgia'; font-size: 42px;\">\n",
    "    Predictive Model For Insurance Claims \n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32e30ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    p {\n",
       "        font-family: 'Arial' ,sans-serif;\n",
       "        font-size: 18px;\n",
       "        line-height: 1.5;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Arial';\n",
       "        font-size: 24px;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "# Define custom styles for the notebook\n",
    "HTML('''\n",
    "<style>\n",
    "    p {\n",
    "        font-family: 'Arial' ,sans-serif;\n",
    "        font-size: 18px;\n",
    "        line-height: 1.5;\n",
    "    }\n",
    "    h1 {\n",
    "        font-family: 'Arial';\n",
    "        font-size: 24px;\n",
    "    }\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6cf7f",
   "metadata": {},
   "source": [
    "# Author's Note\n",
    "<p style=\"font-family: 'Arial'; font-size: 18px; line-height: 1.5;\">\n",
    "This article is designed for both students and anyone interested in understanding practical data science methods. <b>If you're looking for a quick summary, skip ahead to the Results (3min) section for a concise overview of the best-performing methods. </b>\n",
    "\n",
    "The report takes a straightforward, accessible approach, aiming to cover solutions that handle 90% of real-world issues without needing complex work. For those interested in more advanced, high-accuracy methods, please feel free to explore the <b>\"Reading Reference\"</b> section, which includes literature discussing more recent and intricate methodologies.\n",
    "\n",
    "For those diving deeper, I hope this article provides valuable insights to support your learning journey. Thank you for your interest, and I welcome any feedback, suggestions, or ideas for improvement. Feel free to reach out at maverick.article@gmail.com.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e2c50",
   "metadata": {},
   "source": [
    "# Overview (2min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a27bf",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.5;\">\n",
    "    In the world of insurance, claims are rarely made, and for many policyholders, they may never be made at all. The chances of a claim occurring are near zero. This fundamental challenge of insurance claims makes it difficult for insurance companies to create an algorithm to predict the likelihood of an individual to make a claim or not.\n",
    "    <br></br>\n",
    "    This scenario suffers from what is known as classification imbalance (CI), where there is not enough information in certain classes for machine learning algorithms to effectively predict outcomes. Where there is insufficient data in certain classes, the algorithm's performance would suffer. To give an example, assuming the actual insurance claims are only 2% of all premiums, we could have just written down this code:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d636b00",
   "metadata": {},
   "source": [
    "```{python}\n",
    "claims=\"no insurance claims\"\n",
    "claims\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fea264",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Arial', sans-serif; font-size: 18px; line-height: 1.7;\">\n",
    "And just like that, we would achieve a  data scientist's dream- a model boasting 98% accuracy. A very impressive result, right? I would have receive call from all insurance company CEO, each eager to bring that magic into their company. But here’s the reality check: while we might achieve high metrics, it’s often at the cost of deeper insight and real-world applicability. In truth, we’re not only making life a bit more challenging for those tasked with implementation but are also learning that even the most ‘accurate’ model isn’t always the best fit. Funny enough, after all this, we’re still refining our craft, still waiting to get that call for the next big opportunity.\n",
    "<br></br>\n",
    "</p>\n",
    "\n",
    "### Reading Reference (include complex-algorithm not discussed here)\n",
    "* [A survey on single and multi omics data mining methods in cancer data classification](https://www.sciencedirect.com/science/article/pii/S1532046420300939)\n",
    "* [A comprehensive review of object detection with deep learning](https://www.sciencedirect.com/science/article/pii/S1051200422004298) \n",
    "* [Learning From Imbalance Data](https://www.researchgate.net/publication/224541268_Learning_from_Imbalanced_Data) \n",
    "* [Classification with class imbalance problem: A review](https://www.researchgate.net/publication/288228469_Classification_with_class_imbalance_problem_A_review)\n",
    "* [Frank Harrell: Classification vs. Prediction](https://www.fharrell.com/post/classification/)\n",
    "* [PRO-SMOTEBoost: An adaptive SMOTEBoost probabilistic algorithm for rebalancing and improving imbalanced data classification](https://www.sciencedirect.com/science/article/pii/S0020025524014622#:~:text=SMOTEBoost%20rebalances%20many%20highly%20and,changing%20the%20update%20weights%20and)\n",
    "* [Improving k Nearest Neighbor with Exemplar Generalization for Imbalanced Classification](https://link.springer.com/chapter/10.1007/978-3-642-20847-8_27). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f31ea2",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Dataset (1min)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68aad5a",
   "metadata": {},
   "source": [
    "The dataset for car insurance model is collected from [statso.io](https://statso.io/car-insurance-modelling-case-study/) . Below is the feature and the description of the feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c375ec",
   "metadata": {},
   "source": [
    "| Feature                                                                                       | Description                                                                                                                                                                      |\n",
    "|-----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| policy_id                                                                                     | Unique identifier for the insurance policy.                                                                                                                                      |\n",
    "| subscription_length                                                                           | The duration for which the insurance policy is active.                                                                                                                           |\n",
    "| customer_age                                                                                  | Age of the insurance policyholder, which can influence the likelihood of claims.                                                                                                 |\n",
    "| vehicle_age                                                                                   | Age of the vehicle insured, which may affect the probability of claims due to factors like wear and tear.                                                                        |\n",
    "| model                                                                                         | The model of the vehicle, which could impact the claim frequency due to model-specific characteristics.                                                                          |\n",
    "| fuel_type                                                                                     | Type of fuel the vehicle uses (e.g., Petrol, Diesel, CNG), which might influence the risk profile and claim likelihood.                                                          |\n",
    "| max_torque, max_power                                                                         | Engine performance characteristics that could relate to the vehicle’s mechanical condition and claim risks.                                                                      |\n",
    "| engine_type                                                                                   | The type of engine, which might have implications for maintenance and claim rates.                                                                                               |\n",
    "| displacement, cylinder                                                                        | Specifications related to the engine size and construction, affecting the vehicle’s performance and potentially its claim history.                                               |\n",
    "| region_code                                                                                   | The code representing the geographical region of the policyholder, as claim patterns can vary regionally.                                                                        |\n",
    "| region_density                                                                                | Population density of the policyholder’s region, which could correlate with accident and claim frequencies.                                                                      |\n",
    "| airbags                                                                                       | The number of airbags in the vehicle, indicating safety level which can influence claim probability.                                                                             |\n",
    "| is_esc (Electronic Stability Control), is_adjustable_steering, is_tpms (Tire Pressure Monitoring System) | Features that enhance vehicle safety and could potentially reduce the likelihood of claims.                                                                  |\n",
    "| is_parking_sensors, is_parking_camera                                                         | Parking aids that might affect the probability of making a claim, especially in urban areas.                                                                                     |\n",
    "| rear_brakes_type                                                                              | Type of rear brakes, which could be related to the vehicle’s stopping capability and safety.                                                                                     |\n",
    "| Various binary indicators (Yes/No) for specific vehicle amenities and safety features         | Features like steering_type, turning_radius, length, width, gross_weight, etc., which together build a profile of the vehicle’s characteristics and its associated risk factors. |\n",
    "| claim_status   | Indicates whether a claim was made (1) or not (0), which is the dependent variable the model aims to predict.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0051e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>subscription_length</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>region_code</th>\n",
       "      <th>region_density</th>\n",
       "      <th>segment</th>\n",
       "      <th>model</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>max_torque</th>\n",
       "      <th>...</th>\n",
       "      <th>is_brake_assist</th>\n",
       "      <th>is_power_door_locks</th>\n",
       "      <th>is_central_locking</th>\n",
       "      <th>is_power_steering</th>\n",
       "      <th>is_driver_seat_height_adjustable</th>\n",
       "      <th>is_day_night_rear_view_mirror</th>\n",
       "      <th>is_ecw</th>\n",
       "      <th>is_speed_alert</th>\n",
       "      <th>ncap_rating</th>\n",
       "      <th>claim_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POL045360</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>41</td>\n",
       "      <td>C8</td>\n",
       "      <td>8794</td>\n",
       "      <td>C2</td>\n",
       "      <td>M4</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>250Nm@2750rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL016745</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>35</td>\n",
       "      <td>C2</td>\n",
       "      <td>27003</td>\n",
       "      <td>C1</td>\n",
       "      <td>M9</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>200Nm@1750rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POL007194</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>44</td>\n",
       "      <td>C8</td>\n",
       "      <td>8794</td>\n",
       "      <td>C2</td>\n",
       "      <td>M4</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>250Nm@2750rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POL018146</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>44</td>\n",
       "      <td>C10</td>\n",
       "      <td>73430</td>\n",
       "      <td>A</td>\n",
       "      <td>M1</td>\n",
       "      <td>CNG</td>\n",
       "      <td>60Nm@3500rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POL049011</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>C13</td>\n",
       "      <td>5410</td>\n",
       "      <td>B2</td>\n",
       "      <td>M5</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>200Nm@3000rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_id  subscription_length  vehicle_age  customer_age region_code  \\\n",
       "0  POL045360                  9.3          1.2            41          C8   \n",
       "1  POL016745                  8.2          1.8            35          C2   \n",
       "2  POL007194                  9.5          0.2            44          C8   \n",
       "3  POL018146                  5.2          0.4            44         C10   \n",
       "4  POL049011                 10.1          1.0            56         C13   \n",
       "\n",
       "   region_density segment model fuel_type     max_torque  ... is_brake_assist  \\\n",
       "0            8794      C2    M4    Diesel  250Nm@2750rpm  ...             Yes   \n",
       "1           27003      C1    M9    Diesel  200Nm@1750rpm  ...              No   \n",
       "2            8794      C2    M4    Diesel  250Nm@2750rpm  ...             Yes   \n",
       "3           73430       A    M1       CNG   60Nm@3500rpm  ...              No   \n",
       "4            5410      B2    M5    Diesel  200Nm@3000rpm  ...              No   \n",
       "\n",
       "  is_power_door_locks  is_central_locking is_power_steering  \\\n",
       "0                 Yes                 Yes               Yes   \n",
       "1                 Yes                 Yes               Yes   \n",
       "2                 Yes                 Yes               Yes   \n",
       "3                  No                  No               Yes   \n",
       "4                 Yes                 Yes               Yes   \n",
       "\n",
       "  is_driver_seat_height_adjustable is_day_night_rear_view_mirror is_ecw  \\\n",
       "0                              Yes                            No    Yes   \n",
       "1                              Yes                           Yes    Yes   \n",
       "2                              Yes                            No    Yes   \n",
       "3                               No                            No     No   \n",
       "4                               No                            No    Yes   \n",
       "\n",
       "  is_speed_alert ncap_rating  claim_status  \n",
       "0            Yes           3             0  \n",
       "1            Yes           4             0  \n",
       "2            Yes           3             0  \n",
       "3            Yes           0             0  \n",
       "4            Yes           5             0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'DATA SCIENCE/Insurance claims data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bb9ed49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58592 entries, 0 to 58591\n",
      "Data columns (total 41 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   policy_id                         58592 non-null  object \n",
      " 1   subscription_length               58592 non-null  float64\n",
      " 2   vehicle_age                       58592 non-null  float64\n",
      " 3   customer_age                      58592 non-null  int64  \n",
      " 4   region_code                       58592 non-null  object \n",
      " 5   region_density                    58592 non-null  int64  \n",
      " 6   segment                           58592 non-null  object \n",
      " 7   model                             58592 non-null  object \n",
      " 8   fuel_type                         58592 non-null  object \n",
      " 9   max_torque                        58592 non-null  object \n",
      " 10  max_power                         58592 non-null  object \n",
      " 11  engine_type                       58592 non-null  object \n",
      " 12  airbags                           58592 non-null  int64  \n",
      " 13  is_esc                            58592 non-null  object \n",
      " 14  is_adjustable_steering            58592 non-null  object \n",
      " 15  is_tpms                           58592 non-null  object \n",
      " 16  is_parking_sensors                58592 non-null  object \n",
      " 17  is_parking_camera                 58592 non-null  object \n",
      " 18  rear_brakes_type                  58592 non-null  object \n",
      " 19  displacement                      58592 non-null  int64  \n",
      " 20  cylinder                          58592 non-null  int64  \n",
      " 21  transmission_type                 58592 non-null  object \n",
      " 22  steering_type                     58592 non-null  object \n",
      " 23  turning_radius                    58592 non-null  float64\n",
      " 24  length                            58592 non-null  int64  \n",
      " 25  width                             58592 non-null  int64  \n",
      " 26  gross_weight                      58592 non-null  int64  \n",
      " 27  is_front_fog_lights               58592 non-null  object \n",
      " 28  is_rear_window_wiper              58592 non-null  object \n",
      " 29  is_rear_window_washer             58592 non-null  object \n",
      " 30  is_rear_window_defogger           58592 non-null  object \n",
      " 31  is_brake_assist                   58592 non-null  object \n",
      " 32  is_power_door_locks               58592 non-null  object \n",
      " 33  is_central_locking                58592 non-null  object \n",
      " 34  is_power_steering                 58592 non-null  object \n",
      " 35  is_driver_seat_height_adjustable  58592 non-null  object \n",
      " 36  is_day_night_rear_view_mirror     58592 non-null  object \n",
      " 37  is_ecw                            58592 non-null  object \n",
      " 38  is_speed_alert                    58592 non-null  object \n",
      " 39  ncap_rating                       58592 non-null  int64  \n",
      " 40  claim_status                      58592 non-null  int64  \n",
      "dtypes: float64(3), int64(10), object(28)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49d1b7",
   "metadata": {},
   "source": [
    "## Data Information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc5290",
   "metadata": {},
   "source": [
    "1.The dataset contains 58,592 entries with 41 columns\n",
    "\n",
    "2.There is a mix of binary, numerical, and categorical variable\n",
    "\n",
    "3.The claim_status is the target variable, which is the variable we want to be able to predict whether the claim was made (1) or not made (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "066498a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax = sns.countplot(y='claim_status', data=data, hue='claim_status', palette=['lightblue', 'orange'], legend=False)\n",
    "ax.xaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_axisbelow(True) \n",
    "total = len(data)\n",
    "for bar in ax.patches:\n",
    "    count = bar.get_width()\n",
    "    percentage = 100 * count / total\n",
    "    ax.text(count + (total * 0.005), bar.get_y() + bar.get_height() / 2, \n",
    "            f'{int(count)} ({percentage:.1f}%)', \n",
    "            va='center', fontsize=12, color='black', weight='bold')  # Dark text for visibility\n",
    "\n",
    "for bar in ax.patches:\n",
    "    bar.set_edgecolor('black') \n",
    "    bar.set_linewidth(1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4198aca",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 42px;\">Method (15min)</h1>\n",
    "\n",
    "**If you’re here for a quick overview and prefer to skip the finer details, feel free to jump ahead to the \"Selected Method\" section.**\n",
    "<br></br>\n",
    "\n",
    "For those ready to dive deeper, let’s make this exploration as straightforward as possible. In machine learning, there’s rarely a single “correct” answer. Instead, it’s all about making informed decisions based on available techniques. For addressing class imbalance, studies consistently highlight over-sampling and down-sampling as popular solutions:\n",
    "\n",
    "* **Over-sampling**: This involves replicating examples from the minority class until it balances with the majority class. However, while this increases the number of examples, it does not provide new information, thus not solving the issue of insufficient data.\n",
    "\n",
    "- **Down-sampling**: This involves eliminating examples from the majority class until it is balanced with the minority class. The downside of this approach is the potential loss of important information from the majority class.\n",
    "\n",
    "Though these methods are widely used, research suggests there isn’t a “one-size-fits-all” approach for finding the optimal class distribution. For instance, simply aiming for a 50:50 ratio between classes doesn’t always yield the best results.\n",
    "<br></br>\n",
    "\n",
    "Broadly, there are two approaches to handling class imbalance: data-level methods and algorithm-level methods. Data-level techniques include down-sampling, over-sampling, and SMOTE, while algorithm-level methods may involve strategies like feature selection, kNN, SVM, and ensemble approaches. \n",
    "<br></br>\n",
    "\n",
    "# Data-Level Approach\n",
    "\n",
    "## A. SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "According to the paper [Classification with Class Imbalance Problem: A Review](https://www.researchgate.net/publication/288228469_Classification_with_class_imbalance_problem_A_review), SMOTE offers advantages over basic sampling techniques (like over-sampling and down-sampling):\n",
    "\n",
    "- **Increased Diversity**: SMOTE balances the class distribution while adding variety to the training data, leading to better generalization of the model.\n",
    "\n",
    "- **Preventing Overfitting**: Unlike random oversampling, which duplicates examples, SMOTE generates synthetic, slightly different variations of the minority class. This reduces the risk of overfitting and ensures the model doesn't memorize specific instances.\n",
    "<br></br>\n",
    "However, because SMOTE generates synthetic data that might not perfectly reflect real-world variations, it’s important to use it with caution. While it can be effective, SMOTE’s generated samples may sometimes introduce data points that don’t fully capture the <b>true</b> characteristics of the minority class -— essentially creating “virtual” data that lacks the depth of true minority instances. However, for the sake of this report I would try to incorporate SMOTE\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*CeOd_Wbn7O6kpjSTKTIUog.png\" alt=\"SMOTE Example\" style=\"width:50%; display: block; margin: 0 auto;\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## B. Combining Downsampling and Upweighting\n",
    "\n",
    "According to [Google's Datasets: Imbalanced Datasets](https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets), an effective method is to downsample the majority class and then upweight these downwsampled examples in that downsampled majority class. This process ensures:\n",
    "\n",
    "- **Balancing Influence**: Even though fewer examples from the majority class are used, they carry more weight during training, maintaining the overall balance between classes.\n",
    "\n",
    "- **Reduced Bias**: Upweighting the majority class after downsampling helps reduce prediction bias. This ensures the model's predictions reflect the true distribution of the dataset, avoiding an overemphasis on the minority class.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/data-prep/images/downsampling-upweighting-v5.svg\" alt=\"Downsampling Upweighting\" style=\"width:50%; display: block; margin: 0 auto;\">\n",
    "</p>\n",
    "\n",
    "## C. Feature Selection\n",
    "Feature Selection is a method that involves identifying and retaining only the most relevant features for model training. This process reduces the dataset’s dimensionality, which can improve model performance, interpretability, and training efficiency. In the context of class imbalance, feature selection helps by focusing the model on features that provide the most effect for the minority class.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"feature_selection.png\" alt=\"Downsampling Upweighting\" style=\"width:50%; display: block; margin: 0 auto;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f406b",
   "metadata": {},
   "source": [
    "# Algorithm-Level Approach\n",
    "These enhanced algorithms are modified versions of traditional models, specially designed to be more sensitive to imbalanced classes.\n",
    "## A. Standard Algorithm\n",
    "### 1. Logistic Regression with Class Weights\n",
    "Logistic Regression is a classic, interpretable algorithm that remains effective even with class imbalance by applying class weights. By assigning a higher weight to the minority class, we can ensure it doesn’t get lost in the majority.\n",
    "\n",
    "```{python}\n",
    "PYTHON code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 2. Random Forest with Class Weights\n",
    "An ensemble of decision trees that prioritizes minority class samples when class weights are adjusted\n",
    "\n",
    "```{python}\n",
    "PYTHON Code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 3. Gradient Boosting with scale_pos_weight (XGBoost)\n",
    "XGBoost assigns higher importance to the minority class with the scale_pos_weight parameter, making it a go-to for imbalanced datasets.\n",
    "\n",
    "```{python}\n",
    "PYTHON Code\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "ratio = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "xgb = XGBClassifier(scale_pos_weight=ratio, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 4. CatBoost with Class Weights\n",
    "CatBoost’s class weighting makes it effective on imbalanced data, especially with categorical features, and is often faster than other boosting methods.\n",
    "\n",
    "```{python}\n",
    "PYTHON CODE\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = CatBoostClassifier(class_weights=[1, 10], random_state=42, verbose=0)  # Adjust weights as needed\n",
    "catboost.fit(X_train, y_train)\n",
    "```\n",
    "<br></br>\n",
    "\n",
    "## B. Ensemble Methods\n",
    "\n",
    "Ensemble methods take a team approach to predictions by training multiple models and combining their outputs. This approach can be incredibly powerful for imbalanced datasets. Because ensemble methods allow various models to work together, they offer many possible configurations that can be tailored to the specific needs of the data. In general, ensemble methods consists of 2 main way of approach\n",
    "\n",
    "### 1. Bagging\n",
    "In <b>Bagging</b>, we create multiple smaller subsets from the original dataset by sampling with replacement. Each subset is then used to train a separate model and the predictions from all models are combined for the final decision. Bagging helps reduce prediction variance and improves model stability.\n",
    "\n",
    "### 2. Boosting\n",
    "Boosting takes a different approach by training models sequentially, adjusting the weights of misclassified examples in each iteration. Models focus more on \"hard-to-predict\" instances as boosting progresses. Boosting reduces bias and improves prediction accuracy, especially for challenging cases.\n",
    "\n",
    "<b>Popular Boosting Method:</b>\n",
    "\n",
    "- AdaBoost: Boosting algorithm that adjusts the weights of examples after each iteration.\n",
    "- SMOTEBoost: Combines SMOTE (a method for generating synthetic minority class examples) with boosting, giving minority class more representation.\n",
    "- RUSBoost: Undersamples the majority class before boosting, ensuring a balanced approach\n",
    "- DataBoost-IM \n",
    "- cost-sensitive boosting\n",
    "<br></br>\n",
    "\n",
    "# Hybrid Approaches\n",
    "Hybrid approaches combine different machine learning techniques to tackle class imbalance. These methods might combine data-level (e.g., SMOTE) and algorithm-level (e.g., SVM) approaches or merge boosting with undersampling, creating a “hybrid” model designed to maximize performance on imbalanced data. Since hybrid methods allow for diverse model pairings, they can be customized to fit various imbalanced datasets by maximizing the benefits of each technique involved\n",
    "\n",
    "```{python}\n",
    "PYTHON Code\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "hybrid_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "hybrid_pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "![image.png](https://padraig.ca/wp-content/uploads/2022/02/good-enough.jpg)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d691ab",
   "metadata": {},
   "source": [
    "## When is it Overkill?!?!\n",
    "When handling class imbalance, it’s easy to wonder: should we use data-level and algorithm-level approaches together, or is that just overkill? Isn’t one method alone enough to tackle the issue? Let’s break down why a combination can be valuable—and when it might be too much.\n",
    "\n",
    "For those tackling class imbalance for the first time, it’s tempting to go straight to data-level adjustments (like SMOTE or undersampling) to balance things out. But there’s a catch: if applying a data-level approach doesn’t boost the performance of a simple model, we may be adding noise or over-complicating the dataset for little or no gain. In such cases, data-level approaches might be unnecessary, even redundant.\n",
    "\n",
    "## Why Start Simple?\n",
    "By first applying a data-level technique and running a basic algorithm, we get an initial sense of whether balancing the data improves performance. If the results are underwhelming or don’t significantly impact metrics, it’s likely that the data-level approach isn’t adding value. At this point, moving directly to algorithm-level approaches—like ensemble methods or complex models such as XGBoost—can offer the precision and adaptability we need without altering the data itself.\n",
    "\n",
    "## When Complex Models Make Sense\n",
    "If our dataset is relatively simple or not highly imbalanced, introducing powerful models (e.g., Random Forest, Gradient Boosting) may indeed be overkill. These models come with higher computational costs and are designed to handle more intricate data patterns. However, when the imbalance is severe, and a simple model isn’t cutting it, advanced algorithm-level techniques can make all the difference, ensuring that the minority class gets the attention it deserves.\n",
    "\n",
    "\n",
    "# Performance Measure\n",
    "To evaluate the effectiveness of these methods, we use the following metrics:\n",
    "\n",
    "- **Precision**: Indicates the accuracy of positive predictions.\n",
    "- **Recall**: Measures the model’s ability to identify positive instances.\n",
    "- **F1-Score**: Provides a balance between precision and recall.\n",
    "- **AUC-ROC**: Evaluates the model's ability to distinguish between classes without setting a fixed threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c126bdb",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Selected Method (3min)</h1>\n",
    "\n",
    "## Step 1: Apply Feature Selection and Downsapling-Upweighting Technique, SMOTE\n",
    "I chose this method because it makes more sense to classify the data based on feature and assign weight when doing downsampling, rather than rely on synthetic dataset. I would try SMOTE to see if synthetic data could improve our performance results\n",
    "\n",
    "## Step 2: Simple Algorithm\n",
    "I would like to continue our analysis using a simple algorithm as a baseline before using more complex method. The purpose is to compare how much improvement so it is not computationally expensive in the future. For initial analysis, I would use Logistic Regression\n",
    "\n",
    "\n",
    "## Step 3: Hybrid and Ensemble Algorithm\n",
    "Once the baseline from simple algorithm is done, the analysis proceeds with more complex algorithms to leverage their advanced capabilities in handling imbalanced data. I will use:\n",
    "\n",
    "- **Random Forest with Class Weights**\n",
    "- **Balanced Random Forest (Imbalanced-Learn Library)**\n",
    "- **EasyEnsemble (Imbalanced-Learn Library)**\n",
    "- **RUSBoost (Imbalanced-Learn Library)**\n",
    "- **Gradient Boosting with Custom Class Weights**\n",
    "- **Ensemble Pipeline with Class Weighting**\n",
    "\n",
    "## Step 4:Performance Measure\n",
    "To measure the performance for each method, the evaluation will be performed using precision, recall, F1-Score, and AUC-ROC scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d41b1b",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Data Processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f62dc977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "#categorical variable -> numerical\n",
    "data_copy = data.copy()\n",
    "for column in data_copy.select_dtypes(include=['object']).columns:\n",
    "    data_copy[column] = data_copy[column].astype('category').cat.codes\n",
    "\n",
    "X = data_copy.drop(columns=['claim_status'])\n",
    "y = data_copy['claim_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc9ea3",
   "metadata": {},
   "source": [
    "## Step 1: Feature Selection and Downsampling-Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52fc32",
   "metadata": {},
   "source": [
    "#### Feature-Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2fdd2502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['policy_id', 'subscription_length', 'vehicle_age', 'customer_age',\n",
      "       'region_code', 'region_density', 'model', 'fuel_type', 'max_torque',\n",
      "       'max_power', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',\n",
      "       'rear_brakes_type', 'cylinder', 'steering_type', 'width',\n",
      "       'is_front_fog_lights', 'is_day_night_rear_view_mirror',\n",
      "       'is_speed_alert'],\n",
      "      dtype='object')\n",
      "Shape of X_train after feature selection: (41014, 20)\n",
      "Shape of X_test after feature selection: (17578, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "k = 20 #adjustable, later on will test multiple k\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_train_FS = selector.fit_transform(X_train, y_train)\n",
    "X_test_FS = selector.transform(X_test)\n",
    "\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(\"Shape of X_train after feature selection:\", X_train_FS.shape)\n",
    "print(\"Shape of X_test after feature selection:\", X_test_FS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40210e86",
   "metadata": {},
   "source": [
    "#### Downsampling and Upweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "761d353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of downsampled data: (5248, 40) (5248,)\n",
      "Class weights: {0: 14.630335365853659, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "x_majority = X_train[y_train == 0]\n",
    "x_minority = X_train[y_train == 1]\n",
    "y_majority = y_train[y_train == 0]\n",
    "y_minority = y_train[y_train == 1]\n",
    "\n",
    "X_majority_downsampled, y_majority_downsampled = resample(\n",
    "    x_majority, y_majority, replace=False,  \n",
    "    n_samples=len(x_minority),  \n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "X_train_downsampled = pd.concat([X_majority_downsampled, x_minority])\n",
    "y_train_downsampled = pd.concat([y_majority_downsampled, y_minority])\n",
    "\n",
    "class_weight_DU = {0: len(x_majority) / len(x_minority), 1: 1} # assign more weight to downsampled class\n",
    "\n",
    "print(\"Shapes of downsampled data:\", X_train_downsampled.shape, y_train_downsampled.shape)\n",
    "print(\"Class weights:\", class_weight_DU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70253782-67b1-4fcb-851d-192ae887cb24",
   "metadata": {},
   "source": [
    "#### SMOTE and SMOTENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "996cff43-0358-4969-b157-5ddae043e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "X_train_smote_enn, y_train_smote_enn = smote_enn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa145c",
   "metadata": {},
   "source": [
    "## Step 2: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a211e22b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# fine-tune the ideal class_weight\n",
    "param_grid_1 = {\n",
    "    'class_weight': [{0: 1, 1: w} for w in range(1, 20)]  \n",
    "}\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(log_reg, param_grid_1, scoring='f1', cv=5)  #maximizing f1-score\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "72d37f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.96      0.53      0.68     16454\n",
      "       Claim       0.08      0.64      0.15      1124\n",
      "\n",
      "    accuracy                           0.53     17578\n",
      "   macro avg       0.52      0.58      0.41     17578\n",
      "weighted avg       0.90      0.53      0.64     17578\n",
      "\n",
      "AUC-ROC Score for Logistic Regression: 0.6147759828219469\n",
      "----------------------------------------------------------\n",
      "FeatureSelection-Logistic Regression Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.96      0.53      0.69     16454\n",
      "       Claim       0.09      0.64      0.15      1124\n",
      "\n",
      "    accuracy                           0.54     17578\n",
      "   macro avg       0.52      0.58      0.42     17578\n",
      "weighted avg       0.90      0.54      0.65     17578\n",
      "\n",
      "AUC-ROC Score for Feature-Selected Data: 0.6163189450412171\n",
      "----------------------------------------------------------\n",
      "DownsamplingUpweighting-Logistic Regression Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.94      1.00      0.97     16454\n",
      "       Claim       0.00      0.00      0.00      1124\n",
      "\n",
      "    accuracy                           0.94     17578\n",
      "   macro avg       0.47      0.50      0.48     17578\n",
      "weighted avg       0.88      0.94      0.91     17578\n",
      "\n",
      "AUC-ROC Score for Downsampling-Upweighting Data: 0.6088557791007563\n",
      "----------------------------------------------------------\n",
      "Logistic Regression with SMOTE - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.95      0.71      0.81     16454\n",
      "       Claim       0.09      0.40      0.14      1124\n",
      "\n",
      "    accuracy                           0.69     17578\n",
      "   macro avg       0.52      0.56      0.48     17578\n",
      "weighted avg       0.89      0.69      0.77     17578\n",
      "\n",
      "AUC-ROC Score for Logistic Regression with SMOTE: 0.5792343217606121\n",
      "----------------------------------------------------------\n",
      "Logistic Regression with SMOTEENN - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.94      0.77      0.85     16454\n",
      "       Claim       0.08      0.28      0.12      1124\n",
      "\n",
      "    accuracy                           0.74     17578\n",
      "   macro avg       0.51      0.53      0.48     17578\n",
      "weighted avg       0.88      0.74      0.80     17578\n",
      "\n",
      "AUC-ROC Score for Logistic Regression with SMOTEENN: 0.5638502811894002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression without any data-level approach\n",
    "log_reg = LogisticRegression(class_weight=best_params['class_weight'], max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "# Train Logistic Regression on feature-selected data\n",
    "log_reg_fs = LogisticRegression(class_weight=best_params['class_weight'], max_iter=1000, random_state=42) #assumed class_weight is same with no data-level approach\n",
    "log_reg_fs.fit(X_train_FS, y_train)\n",
    "# Train Logistic Regression on Downsampling-Upweighting method\n",
    "log_reg_DU = LogisticRegression(class_weight=class_weight_DU, max_iter=1000, random_state=42)\n",
    "log_reg_DU.fit(X_train_downsampled, y_train_downsampled)\n",
    "# Train Logistic Regression on SMOTE data\n",
    "log_reg_smote = LogisticRegression(class_weight=None, max_iter=1000, random_state=42)\n",
    "log_reg_smote.fit(X_train_smote, y_train_smote)\n",
    "# Train Logistic Regression on SMOTEENN data\n",
    "log_reg_smote_enn = LogisticRegression(class_weight=None, max_iter=1000, random_state=42)\n",
    "log_reg_smote_enn.fit(X_train_smote_enn, y_train_smote_enn)\n",
    "\n",
    "# Prediction\n",
    "y_pred_log_reg = log_reg.predict(X_test) # without data-level approach\n",
    "y_proba_log_reg = log_reg.predict_proba(X_test)[:, 1]\n",
    "y_pred_fs = log_reg_fs.predict(X_test_FS) # with Feature Selection\n",
    "y_proba_fs = log_reg_fs.predict_proba(X_test_FS)[:, 1]  # Probabilities for 'Claim' class\n",
    "y_pred_DU = log_reg_DU.predict(X_test)\n",
    "y_proba_DU = log_reg_DU.predict_proba(X_test)[:, 1] \n",
    "y_pred_smote = log_reg_smote.predict(X_test)\n",
    "y_proba_smote = log_reg_smote.predict_proba(X_test)[:, 1]\n",
    "y_pred_smote_enn = log_reg_smote_enn.predict(X_test)\n",
    "y_proba_smote_enn = log_reg_smote_enn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# AUC-ROC score\n",
    "auc_roc_log_reg = roc_auc_score(y_test, y_proba_log_reg) # without data-level approach\n",
    "auc_roc_fs = roc_auc_score(y_test, y_proba_fs)\n",
    "auc_roc_DU = roc_auc_score(y_test, y_proba_DU)\n",
    "auc_roc_smote = roc_auc_score(y_test, y_proba_smote)\n",
    "auc_roc_smote_enn = roc_auc_score(y_test, y_proba_smote_enn)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log_reg, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Logistic Regression:\", auc_roc_log_reg)\n",
    "\n",
    "print ('----------------------------------------------------------')\n",
    "\n",
    "#Evaluate FeatureSelection-Logistic Regression\n",
    "print(\"FeatureSelection-Logistic Regression Data:\")\n",
    "print(classification_report(y_test, y_pred_fs, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Feature-Selected Data:\", auc_roc_fs)\n",
    "\n",
    "print ('----------------------------------------------------------')\n",
    "\n",
    "#Evaluate Downsampling Upweighting -Logistic Regression\n",
    "print(\"DownsamplingUpweighting-Logistic Regression Data:\")\n",
    "print(classification_report(y_test, y_pred_DU, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Downsampling-Upweighting Data:\", auc_roc_DU)\n",
    "\n",
    "print ('----------------------------------------------------------')\n",
    "\n",
    "print(\"Logistic Regression with SMOTE - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Logistic Regression with SMOTE:\", auc_roc_smote)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "print(\"Logistic Regression with SMOTEENN - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote_enn, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Logistic Regression with SMOTEENN:\", auc_roc_smote_enn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cb2d3557-244d-4080-a437-c0fcec92bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "claim_status\n",
      "0    38390\n",
      "1     2624\n",
      "Name: count, dtype: int64\n",
      "Class distribution after SMOTE:\n",
      "Counter({0: 38390, 1: 38390})\n",
      "Class distribution after SMOTEENN:\n",
      "Counter({1: 24122, 0: 22589})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# After SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(Counter(y_train_smote))\n",
    "\n",
    "# After SMOTEENN\n",
    "print(\"Class distribution after SMOTEENN:\")\n",
    "print(Counter(y_train_smote_enn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d71c0-af87-4346-b482-ac3b1403f142",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Comments\n",
    "Based on  results, here is some observations for each approach:\n",
    "### 1. Baseline Logistic Regression\n",
    "* High precision (0.96) for <b>no claim</b> indicates the model prediction is mostly accurate for this case. Recall (0.68) indicate there is many false positive claim\n",
    "* AUC-ROC score 0.6148 is quite low, indicating the limited ability of model to distinguish between class effectively\n",
    "### 2. Feature-Selection\n",
    "* precision and recall score is almost identical, AUC-ROC only has slight improvement, indicating selected feature not adding much to model performance\n",
    "* We will not use feature-selection in future model since this is\n",
    "### 3. Downsampling-Upweighting\n",
    "* the model almost predict everything as no-claim since precision and recall score for <b>claim</b> is 0.00\n",
    "\n",
    "\n",
    "# Observation\n",
    "The class imbalance is too severe for logistic regression to handle the model effectively\n",
    "For future approach, we will focus on ensemble model and hybrid model.\n",
    "Since there is no improvement from feature-selection, we would refer to not use it in the future as retaining more feature usually would help to catch pattern in minority class better.\n",
    "\n",
    "Downsampling-upweighting seems to have led to stronger bias towards <b>No Claim</b>, also this method led to loss of valuable information which would be beneficial when we are doing a algorithm-level approach, as it is generally better to have more data. We would not follow-up with Downsampling-Upweighting as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6206d-0067-4082-bc02-7b7bc46b1686",
   "metadata": {},
   "source": [
    "## Step 3 : Algorithm-Level Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "713215c7-9786-4eb1-9f58-7a8ea9a0d3ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.95      0.68      0.79     16454\n",
      "       Claim       0.10      0.53      0.17      1124\n",
      "\n",
      "    accuracy                           0.67     17578\n",
      "   macro avg       0.53      0.60      0.48     17578\n",
      "weighted avg       0.90      0.67      0.75     17578\n",
      "\n",
      "AUC-ROC Score for Baseline: 0.6508755456276898\n",
      "----------------------------------------------------------\n",
      "Random Forest with SMOTE - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.96      0.38      0.54     16454\n",
      "       Claim       0.08      0.76      0.14      1124\n",
      "\n",
      "    accuracy                           0.40     17578\n",
      "   macro avg       0.52      0.57      0.34     17578\n",
      "weighted avg       0.90      0.40      0.52     17578\n",
      "\n",
      "AUC-ROC Score for SMOTE: 0.6058715887320069\n",
      "----------------------------------------------------------\n",
      "Random Forest with SMOTEENN - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.95      0.52      0.67     16454\n",
      "       Claim       0.08      0.63      0.14      1124\n",
      "\n",
      "    accuracy                           0.53     17578\n",
      "   macro avg       0.52      0.57      0.41     17578\n",
      "weighted avg       0.90      0.53      0.64     17578\n",
      "\n",
      "AUC-ROC Score for SMOTEENN: 0.6061094728882894\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],  # Adjust as needed\n",
    "    'max_depth': [10, 15],       # Depth of the tree\n",
    "    'min_samples_split': [2, 5], # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [2, 4],  # Minimum samples in a leaf node\n",
    "    'class_weight': [{0: 1, 1: w} for w in [15, 20]], # Class weights for imbalance\n",
    "    'criterion': ['gini', 'entropy'], # Splitting criteria\n",
    "    'max_features': ['sqrt', 'log2']  # Number of features considered at each split\n",
    "}\n",
    "\n",
    "# Use F1-score for the 'Claim' class as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Baseline model without any data-level approach\n",
    "grid_search_baseline = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_baseline.fit(X_train, y_train)\n",
    "best_rf_baseline = grid_search_baseline.best_estimator_\n",
    "\n",
    "# Model with SMOTE\n",
    "grid_search_smote = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_smote.fit(X_train_smote, y_train_smote)\n",
    "best_rf_smote = grid_search_smote.best_estimator_\n",
    "\n",
    "# Model with SMOTEENN\n",
    "grid_search_smoteenn = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_smoteenn.fit(X_train_smote_enn, y_train_smote_enn)\n",
    "best_rf_smoteenn = grid_search_smoteenn.best_estimator_\n",
    "\n",
    "# Evaluate all models on the test set\n",
    "# Baseline\n",
    "y_pred_baseline = best_rf_baseline.predict(X_test)\n",
    "y_proba_baseline = best_rf_baseline.predict_proba(X_test)[:, 1]\n",
    "auc_roc_baseline = roc_auc_score(y_test, y_proba_baseline)\n",
    "\n",
    "print(\"Baseline Random Forest - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Baseline:\", auc_roc_baseline)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "# SMOTE\n",
    "y_pred_smote = best_rf_smote.predict(X_test)\n",
    "y_proba_smote = best_rf_smote.predict_proba(X_test)[:, 1]\n",
    "auc_roc_smote = roc_auc_score(y_test, y_proba_smote)\n",
    "\n",
    "print(\"Random Forest with SMOTE - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for SMOTE:\", auc_roc_smote)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "# SMOTEENN\n",
    "y_pred_smoteenn = best_rf_smoteenn.predict(X_test)\n",
    "y_proba_smoteenn = best_rf_smoteenn.predict_proba(X_test)[:, 1]\n",
    "auc_roc_smoteenn = roc_auc_score(y_test, y_proba_smoteenn)\n",
    "\n",
    "print(\"Random Forest with SMOTEENN - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smoteenn, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for SMOTEENN:\", auc_roc_smoteenn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "72a11acb-02cb-4823-8546-254d191af6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\imblearn\\ensemble\\_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\imblearn\\ensemble\\_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50, 'sampling_strategy': 0.8}\n",
      "Best F1-Score for Claim: 0.16935884378652327\n",
      "Classification Report for Optimized Balanced Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.95      0.71      0.81     16454\n",
      "       Claim       0.10      0.48      0.17      1124\n",
      "\n",
      "    accuracy                           0.70     17578\n",
      "   macro avg       0.53      0.59      0.49     17578\n",
      "weighted avg       0.90      0.70      0.77     17578\n",
      "\n",
      "AUC-ROC Score for Optimized Balanced Random Forest: 0.6530131776846224\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "brf = BalancedRandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500, 1000],\n",
    "    'max_depth': [None, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'sampling_strategy': ['auto', 0.8, 0.6, 0.5],\n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "grid_search = GridSearchCV(estimator=brf, param_grid=param_grid, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_brf = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1-Score for Claim:\", grid_search.best_score_)\n",
    "\n",
    "# Prediction for Balanced Random Forest (Imbalanced-Learn Library)\n",
    "y_pred = best_brf.predict(X_test)\n",
    "y_proba = best_brf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Display classification report and AUC-ROC score\n",
    "print(\"Classification Report for Optimized Balanced Random Forest:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Optimized Balanced Random Forest:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "70cdbbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Parameters: {'n_estimators': 50, 'replacement': True, 'sampling_strategy': 0.8}\n",
      "Best F1-Score for Claim: 0.1608324467178801\n",
      "Classification Report for Optimized EasyEnsemble:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.95      0.71      0.81     16454\n",
      "       Claim       0.10      0.47      0.16      1124\n",
      "\n",
      "    accuracy                           0.70     17578\n",
      "   macro avg       0.53      0.59      0.49     17578\n",
      "weighted avg       0.90      0.70      0.77     17578\n",
      "\n",
      "AUC-ROC Score for Optimized EasyEnsemble: 0.640188493792897\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score, roc_auc_score\n",
    "\n",
    "# Define the EasyEnsembleClassifier\n",
    "easy_ensemble = EasyEnsembleClassifier(random_state=42)\n",
    "\n",
    "# Set up the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50],       # Number of base classifiers\n",
    "    'sampling_strategy': ['auto', 0.8, 0.6],  # Sampling strategies for balancing\n",
    "    'replacement': [True, False]       # Whether to sample with replacement\n",
    "}\n",
    "\n",
    "# Define the scoring metric (F1-score for the Claim class)\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=easy_ensemble,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_easy_ensemble = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters and F1-score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1-Score for Claim:\", grid_search.best_score_)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_easy = best_easy_ensemble.predict(X_test)\n",
    "y_proba_easy = best_easy_ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for Optimized EasyEnsemble:\")\n",
    "print(classification_report(y_test, y_pred_easy, target_names=['No Claim', 'Claim']))\n",
    "\n",
    "# AUC-ROC Score\n",
    "print(\"AUC-ROC Score for Optimized EasyEnsemble:\", roc_auc_score(y_test, y_proba_easy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e2f357c4-2e97-4f72-9d65-ff3de08bb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters for RUSBoost: {'learning_rate': 0.1, 'n_estimators': 100, 'sampling_strategy': 0.8}\n",
      "Best F1-Score for Claim: 0.16378848995818363\n",
      "Classification Report for Optimized RUSBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.95      0.73      0.82     16454\n",
      "       Claim       0.10      0.44      0.16      1124\n",
      "\n",
      "    accuracy                           0.71     17578\n",
      "   macro avg       0.53      0.59      0.49     17578\n",
      "weighted avg       0.90      0.71      0.78     17578\n",
      "\n",
      "AUC-ROC Score for Optimized RUSBoost: 0.6389878263006064\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer, f1_score\n",
    "\n",
    "# Initialize RUSBoostClassifier\n",
    "rus_boost = RUSBoostClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],       # Number of boosting iterations\n",
    "    'learning_rate': [0.01, 0.1, 0.5],   # Learning rate for boosting\n",
    "    'sampling_strategy': ['auto', 0.8, 0.6]  # Undersampling strategy\n",
    "}\n",
    "\n",
    "# Define the scoring metric for optimization (F1-score for minority class)\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rus_boost,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator and parameters\n",
    "best_rus_boost = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters and corresponding F1-score\n",
    "print(\"Best Parameters for RUSBoost:\", best_params)\n",
    "print(\"Best F1-Score for Claim:\", grid_search.best_score_)\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_rusboost = best_rus_boost.predict(X_test)\n",
    "y_proba_rusboost = best_rus_boost.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Classification Report for Optimized RUSBoost:\")\n",
    "print(classification_report(y_test, y_pred_rusboost, target_names=['No Claim', 'Claim']))\n",
    "\n",
    "# AUC-ROC Score\n",
    "auc_roc_rusboost = roc_auc_score(y_test, y_proba_rusboost)\n",
    "print(\"AUC-ROC Score for Optimized RUSBoost:\", auc_roc_rusboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1ee636a1-ef79-44da-b2c3-0d303d60c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maver\\anaconda3\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best F1-Score for Claim: 0.016574500185105263\n",
      "Classification Report for Optimized Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.94      1.00      0.96     16454\n",
      "       Claim       0.10      0.01      0.01      1124\n",
      "\n",
      "    accuracy                           0.93     17578\n",
      "   macro avg       0.52      0.50      0.49     17578\n",
      "weighted avg       0.88      0.93      0.90     17578\n",
      "\n",
      "AUC-ROC Score for Optimized Gradient Boosting: 0.6022144611506164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer, f1_score\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],       # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.1, 0.2],    # Shrinks contribution of each tree\n",
    "    'max_depth': [3, 5, 7],               # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],      # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],        # Minimum samples required to be a leaf node\n",
    "    'subsample': [0.8, 1.0],              # Fraction of samples used for fitting the individual trees\n",
    "    'max_features': ['sqrt', 'log2'],     # Number of features considered for the best split\n",
    "}\n",
    "\n",
    "# Define the scoring metric for optimization (F1-score for Claim class)\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbc,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator and parameters\n",
    "best_gbc = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters and corresponding F1-score\n",
    "print(\"Best Parameters for Gradient Boosting:\", best_params)\n",
    "print(\"Best F1-Score for Claim:\", grid_search.best_score_)\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_gbc = best_gbc.predict(X_test)\n",
    "y_proba_gbc = best_gbc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Classification Report for Optimized Gradient Boosting:\")\n",
    "print(classification_report(y_test, y_pred_gbc, target_names=['No Claim', 'Claim']))\n",
    "\n",
    "# AUC-ROC Score\n",
    "auc_roc_gbc = roc_auc_score(y_test, y_proba_gbc)\n",
    "print(\"AUC-ROC Score for Optimized Gradient Boosting:\", auc_roc_gbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434f5cb-dab8-4536-8adf-6cf0c2b5882c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "895a9f0e-ef8c-47ff-a689-d2d99c78839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.94      1.00      0.97     16454\n",
      "       Claim       0.14      0.00      0.01      1124\n",
      "\n",
      "    accuracy                           0.94     17578\n",
      "   macro avg       0.54      0.50      0.49     17578\n",
      "weighted avg       0.89      0.94      0.91     17578\n",
      "\n",
      "AUC-ROC Score for Baseline Random Forest: 0.5825928978318504\n",
      "----------------------------------------------------------\n",
      "Feature-Selected Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.94      1.00      0.97     16454\n",
      "       Claim       0.05      0.00      0.00      1124\n",
      "\n",
      "    accuracy                           0.94     17578\n",
      "   macro avg       0.49      0.50      0.48     17578\n",
      "weighted avg       0.88      0.94      0.90     17578\n",
      "\n",
      "AUC-ROC Score for Feature-Selected Random Forest: 0.5818993326374792\n",
      "----------------------------------------------------------\n",
      "Downsampling-Upweighting Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Claim       0.96      0.58      0.72     16454\n",
      "       Claim       0.09      0.61      0.16      1124\n",
      "\n",
      "    accuracy                           0.58     17578\n",
      "   macro avg       0.52      0.59      0.44     17578\n",
      "weighted avg       0.90      0.58      0.68     17578\n",
      "\n",
      "AUC-ROC Score for Downsampling-Upweighting Random Forest: 0.6208475034680964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rf_baseline = RandomForestClassifier(random_state=42)\n",
    "rf_fs = RandomForestClassifier(random_state=42)\n",
    "rf_du = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 1. Baseline: Train and evaluate on original data (without any data-level approach)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = rf_baseline.predict(X_test)\n",
    "y_proba_baseline = rf_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Baseline Random Forest\n",
    "print(\"Baseline Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Baseline Random Forest:\", roc_auc_score(y_test, y_proba_baseline))\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "# 2. Feature Selection: Train and evaluate on feature-selected data\n",
    "rf_fs.fit(X_train_FS, y_train)\n",
    "y_pred_fs = rf_fs.predict(X_test_FS)\n",
    "y_proba_fs = rf_fs.predict_proba(X_test_FS)[:, 1]\n",
    "\n",
    "# Evaluate Feature Selection Random Forest\n",
    "print(\"Feature-Selected Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_fs, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Feature-Selected Random Forest:\", roc_auc_score(y_test, y_proba_fs))\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "# 3. Downsampling-Upweighting: Train and evaluate on downsampled and upweighted data\n",
    "rf_du.fit(X_train_downsampled, y_train_downsampled)\n",
    "y_pred_du = rf_du.predict(X_test)\n",
    "y_proba_du = rf_du.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Downsampling-Upweighting Random Forest\n",
    "print(\"Downsampling-Upweighting Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_du, target_names=['No Claim', 'Claim']))\n",
    "print(\"AUC-ROC Score for Downsampling-Upweighting Random Forest:\", roc_auc_score(y_test, y_proba_du))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf668edb",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Results(3min)</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
